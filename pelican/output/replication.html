<!DOCTYPE html>
<html lang="en">
  <head>
        <title>blog - replication</title>
      <meta charset="utf-8" />
      <meta name="generator" content="Pelican" />
      <meta name="viewport" content="width=device-width, initial-scale=1.0">





  </head>

  <body>
    <header>
      <hgroup><h1><a href="localhost/">blog</a></h1></hgroup>
      <nav><ul>
            <li><a href="localhost/category/notes.html"  aria-current="page" >notes</a></li>
            <li><a href="localhost/category/posts.html" >posts</a></li>
      </ul></nav>
    </header>
    <main>
  <article>
    <header>
      <h2>
        <a href="localhost/replication.html" rel="bookmark"
           title="Permalink to replication">replication</a></h2>
      
    </header>
    <p>Why is replication important? In distributed systems, availability is a primary concern. For data to be available to clients, we need redundancy. So if a copy of data is maintained on multiple nodes. More nodes can serve reads if there are failures, or if a single node is not able to handle the load.</p>
<h2>Leaders and Followers</h2>
<p>The most common type of replication is the leader-based replication. All writes go via one leader and are copied to replicas. The client connects with the leader for all writes. Reads can happen from other nodes. The data can be made available on replicas either synchronously or asynchronously. In the sync scenario, the writes are committed only when they have been copied to all the replicas. This is not performant, since copying writes to all replicas may take forever. In the async replication, write can be committed as long as the leader sees it. In async replication, durability is traded off for speed. If a new follower joins the cluster, it must get all the writes from the leader. This is achieved by maintaining snapshots of data on the leader. All writes post the snapshot creation need to be synced differently. One way of doing this could be to execute all statements since the snapshot. This is not ideal, since some statements are temporal, like select current_time() etc. Another way of achieving this is to read the database log. This can either be the WAL of the database. But those are binary, so results could differ across nodes. To improve on this, a logical replication log is maintained by databases which is used for replication in case of new nodes joining or handling node failures.</p>
<h2>Problems with Replication Lag</h2>
<p>Replication is hard. And doing replication in time is harder. Most databases can only claim, “eventual consistency”. This is a very loosely defined and weak promise. A lot of things can go wrong due to lag in replication. Some of them are well understood and are generally avoided, like Read Your Writes (RYW). In RYW promise, the database makes sure that the user can read their writes. Meaning if the writes are happening via the leader, the database will ensure that reads for that user will happen using only those replicas that are in sync with that user’s writes. Monotonic read is the guarantee, which is weaker than consistency, but stronger than eventual consistency. Here the database will guarantee that if a user has read a certain update in the past, they will continue to read that update. Meaning if there are two replicas, that are not entirely in sync, it won't be the case that the user will do a read from one replica and the next read from the other replica. Another guarantee that can be offered is called Consistent Prefix Reads, it is kind of like Monotonic Reads, but slightly different. If there are two writes w1, and w2 such that w2 can only follow w1, for example, w2 is a user’s response to some user’s question w1 then the person reading should always see w1 followed by w2. Transactions are a way to achieve these guarantees.</p>
<h2>Multi-Leader Replication</h2>
<p>Now there are cases when one leader's replication is not enough. Multiple leaders can participate in the cluster. This is especially true in cases where there are many data centers and each data center has its leader. This case is typical in the scenario of disaster recovery protocol. Another use case is a client doing offline writes. For example, a notes app, which only syncs when the device goes online. Here the mobile device and the datacenter where the notes are getting synced can be thought of as two datacenters. Another use case is having users do collaborative editing like in a shared document online. Each of their local copy is essentially a leader in itself which needs to be replicated. Handling conflicts of changes flowing in from multiple leaders is also a challenge in itself. If handled synchronously, it will take forever to resolve and what would the resolution even look like? They need to be resolved asynchronously. Conflict avoidance is a strategy that stops the conflict from happening in the first place. Each user’s update always hits the same data center. This is applicable in the first use case. This breaks down if there is a failure in the user’s data center or the user gets routed to a different data center because of a change in their geolocation. A custom conflict resolution strategy involves passing the responsibility to the application for handling these conflicts. This can be a user-defined function applicable in read or write conflicts depending on the scenario. A multi-leader replication topology can be - circular, start, all to all. The most reliable of these is the all-to-all topology. All other topologies are prone to failure if one leader goes down unexpectedly. </p>
<h2>Leaderless Replication</h2>
<p>Another philosophy is to have no leaders while replication happens. Each user can interact with different replicas and as long as the database can promise to be eventually consistent, there is no problem. However, the story is never simple in software and there are many caveats. Dynamo database popularized this philosophy amongst databases. The core idea is to have separate read quorum and write quorum. If there are n nodes and r and w define the read quorum and write quorum, guarantees can be made depending on the relationship between r+w and n. </p>
<p>Here, “r” is the number of replicas where an update has been replicated to be read, and “w” is the number of replicas where a write has been committed. However, even when r+w &gt; n, there are limitations. Like in the case of sloppy quorum where the set r and the set w are completely disjoint. What happens to concurrent writes? These questions are not answered well so any consistency guarantee needs to be questioned. 
Concurrent writes are hard to detect. Writes that may not be happening at the same exact time may also be concurrent. Two writes which are not aware of each other may be understood as concurrent. To converge to a consistent state, the database needs to adopt a strategy. A commonly used approach is Last Write Wins, also called LWW. This may provide consistency but can come at the cost of durability. “Happens before relationship” between two write events need to be established, to resolve conflicts or to converge to a consistent state. An algorithm that assigns version numbers to writes can be adopted to achieve this. Version vector is the name of the family of algorithms in use to solve this.</p>
    <footer>
      <p>Published: <time datetime="2024-12-26T00:00:00+05:30">
        Thu 26 December 2024
      </time></p>
        <p>
          Category: <a href="localhost/category/notes.html">notes</a>
        </p>
    </footer>
  </article>
    </main>
    <footer>
      <address>
        Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>,
        which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
      </address>
    </footer>
  </body>
</html>